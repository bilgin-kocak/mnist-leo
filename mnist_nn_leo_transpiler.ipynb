{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training MNIST Neural Network Model"
      ],
      "metadata": {
        "id": "HQz6z4IB7CLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_I3Vr2H7ee2",
        "outputId": "85dae652-8f6e-463f-f278-e6b12b32f808"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting tensorflow-model-optimization',\n",
              " '  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/241.2 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m41.0/241.2 kB\\x1b[0m \\x1b[31m1.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m241.2/241.2 kB\\x1b[0m \\x1b[31m3.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)',\n",
              " 'Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)',\n",
              " 'Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.23.5)',\n",
              " 'Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)',\n",
              " 'Installing collected packages: tensorflow-model-optimization',\n",
              " 'Successfully installed tensorflow-model-optimization-0.7.5']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6sgD_0I69ao",
        "outputId": "41441ddf-ca87-4252-e100-1767273ad2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from scipy.ndimage import zoom\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing function\n",
        "def resize_images(images):\n",
        "    return np.array([zoom(image, 0.5) for image in images])\n",
        "\n",
        "# Resize\n",
        "x_train = resize_images(x_train)\n",
        "x_test = resize_images(x_test)\n",
        "\n",
        "# Then reshape\n",
        "x_train = x_train.reshape(60000, 14*14)\n",
        "x_test = x_test.reshape(10000, 14*14)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# normalize to range [0, 1]\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "QHiucl7f7OWh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(14*14,)),\n",
        "    keras.layers.Dense(10, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "IQYcS2BQ7QIH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gefyUmPi7S-A",
        "outputId": "1ce5c800-eb11-45de-d7a3-21cad01563e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.8363 - accuracy: 0.7631 - val_loss: 0.3970 - val_accuracy: 0.8935\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3785 - accuracy: 0.8936 - val_loss: 0.3286 - val_accuracy: 0.9064\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3318 - accuracy: 0.9052 - val_loss: 0.3006 - val_accuracy: 0.9152\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3128 - accuracy: 0.9098 - val_loss: 0.2874 - val_accuracy: 0.9186\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3020 - accuracy: 0.9131 - val_loss: 0.2805 - val_accuracy: 0.9210\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2947 - accuracy: 0.9148 - val_loss: 0.2763 - val_accuracy: 0.9233\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2886 - accuracy: 0.9171 - val_loss: 0.2732 - val_accuracy: 0.9222\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2838 - accuracy: 0.9187 - val_loss: 0.2698 - val_accuracy: 0.9237\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2800 - accuracy: 0.9199 - val_loss: 0.2663 - val_accuracy: 0.9258\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2763 - accuracy: 0.9203 - val_loss: 0.2635 - val_accuracy: 0.9256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize Model Paramaters"
      ],
      "metadata": {
        "id": "XYOoB9Qj7jQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Apply quantization to the layers\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# 'quantize_model' requires a recompile\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sLPalht7l02",
        "outputId": "f9cc2b02-893b-4473-9c60-fb9b0025f73f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLa  (None, 196)               3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrapp  (None, 10)                1975      \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWra  (None, 10)                115       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2093 (8.18 KB)\n",
            "Trainable params: 2080 (8.12 KB)\n",
            "Non-trainable params: 13 (52.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "history = q_aware_model.fit(x_train, y_train,\n",
        "                            epochs=epochs,\n",
        "                            validation_split=0.2)\n",
        "\n",
        "scores, acc = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', scores)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5XkMxKE7U-w",
        "outputId": "3e9fe436-b1c8-4f98-b8d3-e1b6002c9f71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 6s 3ms/step - loss: 0.2749 - accuracy: 0.9211 - val_loss: 0.2629 - val_accuracy: 0.9259\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.2701 - accuracy: 0.9219 - val_loss: 0.2599 - val_accuracy: 0.9263\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2670 - accuracy: 0.9237 - val_loss: 0.2563 - val_accuracy: 0.9269\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.2645 - accuracy: 0.9240 - val_loss: 0.2564 - val_accuracy: 0.9281\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.2620 - accuracy: 0.9247 - val_loss: 0.2539 - val_accuracy: 0.9273\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2609 - accuracy: 0.9244 - val_loss: 0.2549 - val_accuracy: 0.9290\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2581 - accuracy: 0.9252 - val_loss: 0.2534 - val_accuracy: 0.9273\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2570 - accuracy: 0.9262 - val_loss: 0.2537 - val_accuracy: 0.9284\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2543 - accuracy: 0.9261 - val_loss: 0.2549 - val_accuracy: 0.9277\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2540 - accuracy: 0.9268 - val_loss: 0.2503 - val_accuracy: 0.9303\n",
            "Test loss: 0.25907444953918457\n",
            "Test accuracy: 0.929099977016449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "\n",
        "converter_model = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter_model.convert()\n",
        "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "# Indicate that you want to perform default optimizations,\n",
        "# which include quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Define a generator function that provides your test data's numpy arrays\n",
        "def representative_data_gen():\n",
        "  for i in range(500):\n",
        "    yield [x_test[i:i+1]]\n",
        "\n",
        "# Use the generator function to guide the quantization process\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Set the input and output tensors to int8\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"q_aware_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbVGrHXt7tFg",
        "outputId": "61efa15c-7267-491b-b141-1079751fed0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4304"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "jz3NwbNl7vrm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Normalize the input value to int8\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(x_test[0:1], dtype=np.int8)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Perform the inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the result\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iTOZuje7yB7",
        "outputId": "756bdc56-bf2e-43f5-bb99-188a86973ece"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-128 -128 -104  104 -128 -128 -128 -128 -128 -128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(_, _), (x_test_image, y_test_label) = mnist.load_data()\n",
        "\n",
        "# Resize and Normalize x_test_image to int8\n",
        "x_test_image = resize_images(x_test_image)\n",
        "x_test_image_norm = (x_test_image / 255.0 * 255 - 128).astype(np.int8)\n",
        "\n",
        "# Initialize an array to store the predictions\n",
        "predictions = []\n",
        "\n",
        "# Iterate over the test data and make predictions\n",
        "for i in range(len(x_test_image_norm)):\n",
        "    test_image = np.expand_dims(x_test_image_norm[i].flatten(), axis=0)\n",
        "\n",
        "    # Set the value for the input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "\n",
        "    # Run the inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(output)"
      ],
      "metadata": {
        "id": "sIJIfsiw71-G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Leo Neural Network"
      ],
      "metadata": {
        "id": "zuW0-JFM7_q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object with all tensors\n",
        "#(an input + all weights and biases)\n",
        "tensors = {\n",
        "    \"input\": x_test_image[8].flatten(),\n",
        "    \"fc1_weights\": interpreter.get_tensor(1),\n",
        "    \"fc1_bias\": interpreter.get_tensor(2),\n",
        "    \"fc2_weights\": interpreter.get_tensor(4),\n",
        "    \"fc2_bias\": interpreter.get_tensor(5)\n",
        "}"
      ],
      "metadata": {
        "id": "QIvFsD0U74-9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def nn2leo_converter(tensors):\n",
        "    neurons_per_layer = [196, 10, 10] # specifies NN architecture\n",
        "    scaling_factor = 0 # specifies scaling factor for fixed point numbers\n",
        "    integer_type = \"u32\" # specifies used integer type\n",
        "\n",
        "    inputs = tensors['input']\n",
        "\n",
        "    w1 = np.ones(tensors['input'].shape,dtype=int).reshape(1,-1)\n",
        "    b1 = np.zeros(tensors['input'].shape, dtype=int)\n",
        "    w2 = tensors['fc1_weights']\n",
        "    b2 = tensors['fc1_bias']\n",
        "    w3 = tensors['fc2_weights']\n",
        "    b3 = tensors['fc2_bias']\n",
        "\n",
        "    if(len(neurons_per_layer) < 2 or min(neurons_per_layer) < 1):\n",
        "        print(\"error, invalid input\")\n",
        "\n",
        "    str_list_main = []\n",
        "    str_list_inputs = []\n",
        "\n",
        "    str_main=\"program nn.aleo {\\ntransition main(\"\n",
        "\n",
        "    str_inputs = \"\"\n",
        "\n",
        "    str_list_inputs.append(\"[main]\\n\")\n",
        "\n",
        "    # for i in range(neurons_per_layer[0]):\n",
        "    #     str_main += \"w0\" + str(i)+\": \" + integer_type + \", b0\" + str(i) + \": \" + integer_type + \", \"\n",
        "    #     str_inputs += \"w0\" + str(i) + \": \" + integer_type + \" = 0\"+integer_type+\";\\n\"\n",
        "    #     str_inputs += \"b0\" + str(i) + \": \" + integer_type + \" = 0\"+integer_type+\";\\n\"\n",
        "\n",
        "    # str_list_inputs.append(str_inputs)\n",
        "    # str_inputs = \"\"\n",
        "\n",
        "    # for i in range(1, len(neurons_per_layer)): # current layer\n",
        "    #     for j in range(neurons_per_layer[i-1]): # neuron of previous layer\n",
        "    #         for k in range(neurons_per_layer[i]): # neuron of current layer\n",
        "    #             str_main += \"w\" + str(i) + str(j) + str(k) + \": \" + integer_type + \", \"\n",
        "    #             str_inputs += \"w\" + str(i) + str(j) + str(k) + \": \" + integer_type + \" = 0\"+integer_type+\";\\n\"\n",
        "    #         str_main += \"b\" + str(i) + str(j) + \": \" + integer_type + \", \"\n",
        "    #         str_inputs += \"b\" + str(i) + str(j) + \": \" + integer_type + \" = 0\"+integer_type+\";\\n\"\n",
        "\n",
        "    for i in range(math.ceil(neurons_per_layer[0]/32)):\n",
        "        if i + 1 != math.ceil(neurons_per_layer[0]/32):\n",
        "            str_main += \"inputs\"+str(i+1)+\": [\" + integer_type + \"; 32], \"\n",
        "            str_inputs += \"inputs\"+str(i+1)+\": \" + integer_type + \" = 0\"+integer_type+\";\\n\"\n",
        "        else:\n",
        "            str_main += \"inputs\"+str(i+1)+\": [\" + integer_type + \"; \"+str(neurons_per_layer[0]%32)+\"], \"\n",
        "\n",
        "    str_main = str_main[:-2]\n",
        "    str_list_inputs.append(str_inputs)\n",
        "\n",
        "    str_inputs = \"[registers]\\n\"\n",
        "\n",
        "    str_main += \") -> [\" + integer_type + \"; \" + str(neurons_per_layer[-1]) + \"] {\\n\"\n",
        "\n",
        "    str_list_main.append(str_main)\n",
        "\n",
        "    line = \"\"\n",
        "\n",
        "    # for i in range(neurons_per_layer[0]): # input layer\n",
        "    #     line += \"let neuron0\"+str(i) + \": \" + integer_type + \" = w0_\" + str(i) + \" * input\" + str(i) + \" / \" + str(2**scaling_factor)+ integer_type + \" + b0_\" + str(i) + \";\\n\"\n",
        "\n",
        "    for i in range(neurons_per_layer[0]): # input layer\n",
        "        line += \"let neuron0\"+str(i) + \": \" + integer_type + \" = inputs\" + str((i)//32+1) + f\"[{i%32}{integer_type}];\\n\"\n",
        "\n",
        "\n",
        "\n",
        "    for layer in range(1, len(neurons_per_layer)): # other layers\n",
        "        for i in range(neurons_per_layer[layer]):\n",
        "            line_start = \"let neuron\" + str(layer) + str(i) + \": \" + integer_type + \" = rectified_linear_activation(\"\n",
        "            for j in range(neurons_per_layer[layer-1]):\n",
        "                line_start += \"neuron\" + str(layer-1) + str(j) + \" * w\" + str(layer) +\"_\" + str(j) +\"_\" + str(i) + \" / \" + str(2**scaling_factor)+ integer_type + \" + \"\n",
        "\n",
        "            line_start += \"b\" + str(layer) +\"_\" + str(i) + \");\\n\"\n",
        "            line += line_start\n",
        "\n",
        "    str_list_main.append(line)\n",
        "\n",
        "    line = \"return [\"\n",
        "    str_inputs += \"r0: [\" + integer_type + \"; \" + str(neurons_per_layer[-1]) + \"] = [\"\n",
        "    for i in range(neurons_per_layer[-1]):\n",
        "        line += \"neuron\" + str(len(neurons_per_layer)-1) + str(i) + \", \"\n",
        "    str_inputs += \"0, \"\n",
        "    str_inputs = str_inputs[:-2] + \"];\\n\"\n",
        "\n",
        "    line = line[:-2]\n",
        "    line += \"];}\\n\\n\"\n",
        "    str_list_main.append(line)\n",
        "    str_list_inputs.append(str_inputs)\n",
        "\n",
        "    str_list_main.append(\"function rectified_linear_activation(x: \"+str(integer_type)+\") -> \"+str(integer_type)+\" {\\n\")\n",
        "    str_list_main.append(\"let result: \"+str(integer_type)+\" = 0\" + integer_type + \";\\n\")\n",
        "    str_list_main.append(\"if x > 0\" + integer_type + \" {\\n\")\n",
        "    str_list_main.append(\"result = x;\\n\")\n",
        "    str_list_main.append(\"}\\n\")\n",
        "    str_list_main.append(\"return result;\\n\")\n",
        "    str_list_main.append(\"}\\n\")\n",
        "    str_list_main.append(\"}\")\n",
        "\n",
        "    with open(\"main.leo\", \"w+\") as file:\n",
        "        file.writelines(str_list_main)\n",
        "\n",
        "    with open(\"project.in\", \"w+\") as file:\n",
        "        file.writelines(str_list_inputs)\n",
        "\n",
        "    with open(\"main.leo\", \"r\") as file:\n",
        "        aa = file.read()\n",
        "\n",
        "    temp = \"\\n\".join(aa.split(\"\\n\")[2:])\n",
        "\n",
        "    weights = [w1, w2, w3]\n",
        "    biases = [b1, b2, b3]\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        temp = temp.replace(f\"input{i}\", str(inputs[i])+str(integer_type),1)\n",
        "\n",
        "    for i in range(w1.shape[1]):\n",
        "        temp = temp.replace(f\"w0_{i}\", str(w1[0][i])+str(integer_type), 1)\n",
        "\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(weights[i].shape[0]):\n",
        "            for k in range(weights[i].shape[1]):\n",
        "                temp = temp.replace(f\"w{i}_{k}_{j}\", str(weights[i][j][k])+str(integer_type), 1)\n",
        "                temp = temp.replace(f\"w{i}_{k}_0\", str(weights[i][0][k])+str(integer_type), 1)\n",
        "\n",
        "    for i in range(len(biases)):\n",
        "        for j in range(len(biases[i])):\n",
        "            temp = temp.replace(f\"b{i}_{j}\", str(biases[i][j])+str(integer_type), 1)\n",
        "\n",
        "    first_part = aa.split(\"\\n\")[:2]\n",
        "    last_part = temp.split(\"\\n\")\n",
        "    str_main_list = first_part + last_part\n",
        "    str_main = \"\\n\".join(str_main_list)\n",
        "    ## Write to file\n",
        "    with open(\"main-.leo\", \"w+\") as file:\n",
        "        file.write(str_main)\n",
        "    create_input_file(integer_type, list(inputs))\n",
        "    a = 5\n",
        "\n",
        "# Define a function to chunk the input array in size of 32\n",
        "def chunked_inputs(inputs, chunk_size=32):\n",
        "    return [inputs[i:i + chunk_size] for i in range(0, len(inputs), chunk_size)]\n",
        "def create_input_file(integer_type, inputs):\n",
        "    # Define the input array\n",
        "    # inputs = [1, 2]\n",
        "\n",
        "\n",
        "    str_input_file = \"\"\n",
        "    str_input_file += f\"[main]\\n\"\n",
        "    for idx, chunk in enumerate(chunked_inputs(inputs), start=1):\n",
        "        inputs_str = ','.join(str(i) + str(integer_type) for i in map(str, chunk))  # Convert chunk to string\n",
        "        str_input_file += f'inputs{idx}: [{integer_type}; {len(chunk)}] = [{inputs_str}];\\n'\n",
        "\n",
        "\n",
        "    # Create or open the file to write the inputs\n",
        "    with open('nn.in', 'w') as file:\n",
        "        # Write the [main] section header\n",
        "        file.write(f\"[main]\\n\")\n",
        "        str_input_file + f\"[main]\\n\"\n",
        "\n",
        "        for idx, chunk in enumerate(chunked_inputs(inputs), start=1):\n",
        "            inputs_str = ','.join(str(i) + str(integer_type) for i in map(str, chunk))  # Convert chunk to string\n",
        "            file.write(f'inputs{idx}: [{integer_type}; {len(chunk)}] = [{inputs_str}];\\n')\n",
        "            str_input_file + f'inputs{idx}: [{integer_type}; {len(chunk)}] = [{inputs_str}];\\n'\n",
        "\n",
        "        # # Write the inputs array in the specified format\n",
        "        # inputs_formatted = ','.join(str(i) + str(integer_type) for i in inputs)  # Format array as a comma-separated string\n",
        "        # file.write(f'= [{inputs_formatted}];\\n')\n",
        "    print(\"The file with inputs has been created successfully.\")\n",
        "    return str_input_file"
      ],
      "metadata": {
        "id": "8uY5xrYt8EaS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn2leo_converter(tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTNkhOsq8MVy",
        "outputId": "e7b82029-cf0b-40fa-8004-26fa40f609af"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file with inputs has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download MNIST Dataset"
      ],
      "metadata": {
        "id": "ZFw8QFrg8cnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Function to save dataset images as jpeg files\n",
        "def save_mnist_as_jpeg():\n",
        "    # Load the dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "    # Create directories for training and test sets\n",
        "    os.makedirs('mnist_data/train', exist_ok=True)\n",
        "    os.makedirs('mnist_data/test', exist_ok=True)\n",
        "\n",
        "    # Save training images\n",
        "    for idx, image in enumerate(train_images):\n",
        "        file_path = os.path.join('mnist_data/train', f'train_image_{idx}_{train_labels[idx]}.jpg')\n",
        "        img = Image.fromarray(image)\n",
        "        img.save(file_path, 'JPEG')\n",
        "\n",
        "    # Save test images\n",
        "    for idx, image in enumerate(test_images):\n",
        "        file_path = os.path.join('mnist_data/test', f'test_image_{idx}_{test_labels[idx]}.jpg')\n",
        "        img = Image.fromarray(image)\n",
        "        img.save(file_path, 'JPEG')\n",
        "\n",
        "    print(\"MNIST dataset has been saved as JPEG files.\")\n",
        "\n",
        "# Call the function\n",
        "save_mnist_as_jpeg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jia5wFZP8e6U",
        "outputId": "927daa55-ab25-4edf-95c7-ae663d175907"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset has been saved as JPEG files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1bxMJeR821m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}